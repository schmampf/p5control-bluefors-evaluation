{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67171049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55511f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/dacap/Documents/p5control-bluefors-evaluation/simulation/data/Reference\"\n",
    "files = os.listdir(path)\n",
    "\n",
    "arr = loadmat(f\"{path}/{files[10]}\")\n",
    "\n",
    "V = np.array(arr[\"eV_eff_range\"][0], dtype=\"float64\")\n",
    "I_0 = np.full((len(files) + 1, len(V)), 0, dtype=\"float64\")\n",
    "I_m = np.full((len(files) + 1, 20, len(V)), 0, dtype=\"float64\")\n",
    "tau = np.full(len(files) + 1, 0, dtype=\"float64\")\n",
    "M = np.arange(1, 21)\n",
    "\n",
    "# arr = loadmat(f'{path}/{files[80]}')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSet:\n",
    "    V0: np.ndarray\n",
    "    I0: np.ndarray\n",
    "    Im: Dict[int, np.ndarray]\n",
    "\n",
    "\n",
    "sets: Dict[float, DataSet] = {}\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    arr = loadmat(f\"{path}/{file}\")\n",
    "\n",
    "    sets[float(file[15:19])] = DataSet(\n",
    "        V0=np.real(arr[\"eV_eff_range\"][0]),\n",
    "        I0=np.real(arr[\"I_In\"][:, 0]) / 2,\n",
    "        Im={\n",
    "            int(m): np.real(\n",
    "                arr[\"In_final\"][:, 20 - m] / 2 + arr[\"In_final\"][:, 20 + m] / 2\n",
    "            )\n",
    "            for m in M\n",
    "        },\n",
    "    )\n",
    "\n",
    "# save sets as h5py file\n",
    "\n",
    "with h5py.File(\"DavidOIV.hdf5\", \"w\") as f:\n",
    "    for key, dataset in sets.items():\n",
    "        group = f.create_group(str(key))\n",
    "        group.create_dataset(\"V0\", data=dataset.V0)\n",
    "        group.create_dataset(\"I0\", data=dataset.I0)\n",
    "        for m, Im in dataset.Im.items():\n",
    "            group.create_dataset(f\"Im_{m}\", data=Im)\n",
    "\n",
    "# sorting_indices = np.argsort(tau)\n",
    "# tau = tau[sorting_indices]\n",
    "# I_0= I_0[sorting_indices, :]\n",
    "# I_m = I_m[sorting_indices, :, :]\n",
    "\n",
    "\n",
    "# V = np.concatenate(\n",
    "#     (\n",
    "#     V[:590],\n",
    "#     -np.linspace(.09, .01, 9),\n",
    "#     np.linspace(.01, .09, 9),\n",
    "#     V[590:]\n",
    "#     ),\n",
    "#     )\n",
    "\n",
    "# I_0 = np.concatenate(\n",
    "#     (\n",
    "#     I_0[:,:590],\n",
    "#     np.full((len(files)+1, 18), np.nan),\n",
    "#     I_0[:,590:]\n",
    "#     ),\n",
    "#     axis=1,\n",
    "#     )\n",
    "\n",
    "# I_m = np.concatenate(\n",
    "#     (\n",
    "#     I_m[:,:,:590],\n",
    "#     np.full((len(files)+1, 20, 18), np.nan),\n",
    "#     I_m[:,:,590:]\n",
    "#     ),\n",
    "#     axis=2,\n",
    "#     )\n",
    "\n",
    "# dIdV = np.gradient(I_0, V, axis=1)\n",
    "# dIdV_m = np.gradient(I_m, V, axis=2)\n",
    "\n",
    "# mars = {\n",
    "#     'V': V,\n",
    "#     'tau': tau,\n",
    "#     'I': I_0,\n",
    "#     'dIdV': dIdV,\n",
    "#     'm': m,\n",
    "#     'I_m': I_m,\n",
    "#     'dIdV_m': dIdV_m,\n",
    "# }\n",
    "\n",
    "# with open('DavidOIV.pickle', 'wb') as f:\n",
    "#     pickle.dump(mars, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
